# Release notes

- [Release notes](#release-notes)
  - [Auto-join nodes feature](#auto-join-nodes-feature)
    - [Use case](#use-case)
    - [Issue](#issue)
    - [Change](#change)
      - [How to deploy it](#how-to-deploy-it)
        - [Before start](#before-start)
        - [Requirements](#requirements)
        - [Upgrading an existing cluster](#upgrading-an-existing-cluster)
          - [Terraform part](#terraform-part)
          - [Ansible part](#ansible-part)
        - [Verify](#verify)

This version contains a new feature: **auto-join** nodes.

## Auto-join nodes feature

### Use case

Having a Kubernetes cluster with nodes belonging to an autoscaling group. Node pool has to be able to create and destroy nodes in a dynamic way. New nodes needs to join the Kubernetes cluster without human intervention.

### Issue

Before this feature, an operator had to run an ansible playbook every time a new node was created to join the cluster. This causes some problems in cloud environments. You usually deploy your nodes in an autoscaling group. This means that the number of nodes in a given pool can increase or decrease. It happens in a very dynamic way, it can cause having instances not belonging to the Kubernetes cluster. It’s not only a matter of scale, when cloud providers needs to do some maintenance stuff, they rely in the autoscaling group killing an instance and letting the autoscaling group to recreate it.

### Change

Having nodes following the inmutable infrastructure paradigm makes this feature possible. This means that new Golden AMIs are created to achieve this scenario. New nodes contains all the needed stuff. A minimal configuration is passed via cloud-init script.

#### How to deploy it

##### Before start

First of all, this features modifies the following components:

- [modules/aws-kubernetes](modules/aws-kubernetes)
- [modules/s3-furyagent](modules/s3-furyagent)
- [roles/furyagent](roles/furyagent)
- [roles/kube-control-plane](roles/kube-control-plane)
- [roles/kube-node-common](roles/kube-node-common)
- [roles/kube-worker](roles/kube-worker): *Deleted*

This modifications contains some breaking changes:

- [modules/aws-kubernetes](modules/aws-kubernetes):
  - `s3-bucket-name`: Bucket name containing fury stuff like join tokens: Example: `my-bucket-name`.
  - `join-policy-arn`: Policy ARN needed to pull/push join tokens. You could get it from the `s3-furyagent` module.
  - `alertmanager-hostname`: Alert-Manager hostname used by the cloud-init watcher to notify errors using it’s API. Example: `my-alert-manager-instance.my-company.com`.
  - `kube-worker`: This object list adds a new object attribute: `kube-ami`. `kube-ami` is the **Name** of the AMI you want to use. Take a look to [AMI.md](../AMI.md) to find the correct one.
  - `kube-ami` -> `kube-master-ami`: The `kube-ami` variable has been renamed to `kube-master-ami`.
- [roles/kube-control-plane](roles/kube-control-plane):
  - `s3_bucket_name`: Bucket name containing fury stuff like join tokens: Example: `my-bucket-name`.

##### Requirements

Having version 1.15.4 deployed, change the version to 1.15.4-2 in your `Furyfile.yml` *(only in `aws` modules and roles)*, then:

```bash
$ rm -rf vendor/
$ furyctl install
```

This command downloads this feature in your `vendor` directory.

##### Upgrading an existing cluster

Having the vendor directory upgraded, we can start doing the required modifications.

###### Terraform part

Just modify the `k8s` module with the following attributes:

```diff
--- a/terraform/main.tf
+++ b/terraform/main.tf
@@ -42,7 +42,7 @@ module "k8s" {
   region                             = "${var.aws_region}"
   name                               = "${var.name}"
   env                                = "${var.env}"
-  kube-ami                           = "ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190212.1-*"
+  kube-master-ami                    = "ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190212.1-*"
   kube-master-count                  = 3
   kube-master-type                   = "t3.medium"
   kube-private-subnets               = "${module.vpc.private_subnets}"
@@ -50,6 +50,10 @@ module "k8s" {
   kube-domain                        = "${module.vpc.domain_zone}"
   kube-bastions                      = "${module.vpc.bastion_public_ip}"
   ssh-private-key                    = "${var.ssh-private-key}"
+  s3-bucket-name                     = "sighup-${var.name}-${var.env}-agent"
+  join-policy-arn                    = "${module.prod-furyagent.bucket_policy_join}"
+  alertmanager-hostname              = "alertmanager.test.someone-fury.sighup.io"
+

   kube-lb-internal-domains = [
     "grafana",
@@ -68,6 +72,7 @@ module "k8s" {
       kind     = "infra"
       count    = 2
       type     = "t3.medium"
+      kube-ami = "KFD-Ubuntu-Node-1.15.5-2-*"
     },
   ]
```

**IMPORTANT NOTE**

  Set the `kube-master-ami` to the **current** master AMI name. Otherwise your master will be recreated. It's critical to review the `terraform plan` output before applying any configuration. If you see that your Kubernetes master nodes will be destroyed, stop the upgrade. This means that you changed the `kube-master-ami` value *(renamed from `kube-ami`)*.


Then, review the changes with `terraform plan`:

```bash
$ cd terraform
$ make plan
Terraform will perform the following actions:

 <= module.k8s.data.aws_instances.main
      id:                                        <computed>
      filter.#:                                  "1"
      filter.3199815247.name:                    "tag:aws:autoscaling:groupName"
      filter.3199815247.values.#:                "1"
      filter.3199815247.values.0:                "someone-fury-test-k8s-infra-asg"
      ids.#:                                     <computed>
      instance_state_names.#:                    "1"
      instance_state_names.3190512617:           "running"
      instance_tags.%:                           <computed>
      private_ips.#:                             <computed>
      public_ips.#:                              <computed>

 <= module.k8s.data.template_file.k8s-worker-node
      id:                                        <computed>
      rendered:                                  <computed>
      template:                                  "[${kind}]\n${nodes}\n"
      vars.%:                                    <computed>

  ~ module.k8s.aws_autoscaling_group.main
      launch_configuration:                      "someone-fury-test-k8s-infra-nodes20191030072204934800000001" => "${element(aws_launch_configuration.main.*.name, count.index)}"

  + module.k8s.aws_iam_policy_attachment.join
      id:                                        <computed>
      name:                                      "kubernetes_worker_join"
      policy_arn:                                "${var.join-policy-arn}"
      roles.#:                                   "1"
      roles.2745673947:                          "someone-fury-test-kubernetes-role"

-/+ module.k8s.aws_launch_configuration.main (new resource required)
      id:                                        "someone-fury-test-k8s-infra-nodes20191030072204934800000001" => <computed> (forces new resource)
      associate_public_ip_address:               "false" => "false"
      ebs_block_device.#:                        "0" => <computed>
      ebs_optimized:                             "false" => <computed>
      enable_monitoring:                         "true" => "true"
      iam_instance_profile:                      "someone-fury-test-kubernetes-instance-profile" => "someone-fury-test-kubernetes-instance-profile"
      image_id:                                  "ami-0764a46039d5d5fa5" => "ami-0b99a01290e60924f" (forces new resource)
      instance_type:                             "t3.medium" => "t3.medium"
      key_name:                                  "" => <computed>
      name:                                      "someone-fury-test-k8s-infra-nodes20191030072204934800000001" => <computed>
      name_prefix:                               "someone-fury-test-k8s-infra-nodes" => "someone-fury-test-k8s-infra-nodes"
      root_block_device.#:                       "1" => "1"
      root_block_device.0.delete_on_termination: "true" => "true"
      root_block_device.0.iops:                  "0" => <computed>
      root_block_device.0.volume_size:           "80" => "80"
      root_block_device.0.volume_type:           "gp2" => "gp2"
      security_groups.#:                         "1" => "1"
      security_groups.2920587180:                "sg-0f34202b2dc93f58f" => "sg-0f34202b2dc93f58f"
      user_data:                                 "336b5dd51fa515d0906da0012a8134c666facc4e" => "bc46df15f5f2762fe3d55c260ac58795c3feceb2" (forces new resource)

  + module.prod-furyagent.aws_iam_policy.join
      id:                                        <computed>
      arn:                                       <computed>
      name:                                      "someone-fury-test-furyagent-join"
      path:                                      "/"
      policy:                                    "{\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n         {\n             \"Effect\": \"Allow\",\n             \"Action\": [\n                 \"s3:*\"\n             ],\n            \"Resource\": \"arn:aws:s3:::sighup-someone-fury-test-agent/join/*\"\n         },\n         {\n             \"Effect\": \"Allow\",\n             \"Action\": [\n                 \"s3:ListBucket\",\n                 \"s3:GetBucketLocation\"\n             ],\n            \"Resource\": \"arn:aws:s3:::sighup-someone-fury-test-agent\"\n         }\n     ]\n}\n"


Plan: 3 to add, 1 to change, 1 to destroy.
```

Once reviewed the plan, apply the configuration:

```bash
$ make run
```

And wait until completion.


###### Ansible part

The only file that needs some modifications is the `cluster.yml` playbook.

```diff
--- a/ansible/cluster.yml
+++ b/ansible/cluster.yml
@@ -1,12 +1,12 @@
 - name: Kubernetes node preparation
-  hosts: master,nodes
+  hosts: master
   become: true
   roles:
     - aws/kube-node-common

 - name: Installing and configuring furyagent
   hosts: master
   become: true
   vars:
     furyagent_configure_master: true
     furyagent_configure_etcd: true
@@ -25,18 +25,16 @@
 - name: Control plane configuration
   hosts: master
   become: true
   vars:
     kubernetes_api_SAN:
       - '{{ public_lb_address }}'
     kubernetes_kubeconfig_path: '../secrets/users'
     kubernetes_cluster_name: 'fury-test'
     kubernetes_users_org: sighup
     kubernetes_control_plane_address: '{{control_plane_endpoint}}:6443'
+    kubernetes_version: '1.15.4'
+  vars_files:
+    - '../secrets/fury.yml'
   roles:
     - aws/kube-control-plane

-- name: Kubernetes join nodes
-  hosts: nodes
-  become: true
-  roles:
-    - aws/kube-worker
```

There is a couple of changes in the `cluster.yml` playbook:

- The `Kubernetes node preparation` now is only needed by the master nodes. You can even delete the complete `Kubernetes node preparation` if you are using an inmutable Master AMI *(installation from scratch)*.
- The `Control plane configuration` needs the `fury.yml` var file containing the `s3_bucket_name` variable. It is also a requirement to pass the `kubernetes_version` as an input variable. It has to be the same version as the actually deployed version.
- The `Kubernetes join nodes` is not longer needed.

Once you modified the playbook, execute it!

```bash
$ cd ansible
$ ansible-playbook cluster.yml
```

Wait until completion.

##### Verify

To test it, just terminate *(from aws web console or aws command line tool)* the two infra nodes instances. The autoscaling group will create two new nodes with the new AMI. Wait a little bit and new infra nodes will join the cluster:

```bash
$ kubectl get nodes
NAME                                          STATUS   ROLES    AGE    VERSION
ip-10-100-10-110.eu-west-1.compute.internal   Ready    infra    30m    v1.15.5
ip-10-100-10-145.eu-west-1.compute.internal   Ready    master   156m   v1.15.4
ip-10-100-11-12.eu-west-1.compute.internal    Ready    master   156m   v1.15.4
ip-10-100-11-214.eu-west-1.compute.internal   Ready    infra    74m    v1.15.5
ip-10-100-12-64.eu-west-1.compute.internal    Ready    master   156m   v1.15.4
```

